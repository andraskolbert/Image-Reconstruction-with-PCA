#Creating named list for the dropdown menu
ls_choices.list <- as.list(picfiles)
names(ls_choices.list) <- picnames
ls_choices.list
#Updateing the dropdown list with the created named list - not working yet
### !!problem is the condition, that's not following the values - links yet !!###
#updateSelectInput(session, "image", choices = ls_choices.list)
# Change values for input$inSelect
var <-switch(input$image,
"lisbon" = "pics/LisbonInput.jpg",
"monalisa" = "pics/MonaLisaInput.jpg",
"user" = "pics/UserInput.jpg"
)
# loading picture
original=readJPEG(var)
R=original[,,1]
G=original[,,2]
B=original[,,3]
#Updating the maximum number of PCas to use
updateSliderInput(session, "numPCs", max =dim(original)[2])
#output$testing <- renderDataTable(picnames)
# Do the same process separatly for each channel
#R=original[,,1]
#G=original[,,2]
#B=original[,,3]
# Compute a correlation or covariance matrix
# and its eigen vectors
#Number of principal components to use:
k = input$numPCs
### Red
r <-switch(input$CorCov,
"cor" = cor(R),
"cov" = cov(R)
)
g=eigen(r)
Rv=g$vectors[,1:k]
###Green
r <-switch(input$CorCov,
"cor" = cor(G),
"cov" = cov(G)
)
g=eigen(r)
Gv=g$vectors[,1:k]
###Blue
r <-switch(input$CorCov,
"cor" = cor(B),
"cov" = cov(B)
)
g=eigen(r)
Bv=g$vectors[,1:k]
###########################
#output theory for R,G or B
A <- switch(input$RGB,
"r" = R,
"g" = G,
"b" = B)
g <- switch(input$CorCov,
"cor" = eigen(cor(A)),
"cov" = eigen(cov(A)))
#perentage of total variation explained by the components
perc_exp<-g$values/NCOL(A)
cum_exp<-c(perc_exp[1], sum(perc_exp[1:2]), sum(perc_exp[1:3]),
sum(perc_exp[1:4]),sum(perc_exp[1:5]),sum(perc_exp[1:6]),
sum(perc_exp[1:7]),sum(perc_exp[1:8]),sum(perc_exp[1:9]),
sum(perc_exp[1:10]),sum(perc_exp[1:11]),sum(perc_exp[1:12]),
sum(perc_exp[1:13]),sum(perc_exp[1:14]),sum(perc_exp[1:15]))
table_exp<-cbind(values=g$values[1:15], variance_explained=perc_exp[1:15],
cummulated_variance_explained=cum_exp)
#output for the app:
output$table_exp <- renderTable({
table_exp
})
#just for testing the variable, can be deleted in the end
#writing location
output$testing <- renderPlot({
plot(g$values, type='b')
})
############
#screeplot to see the elbow
#output for the app:
lambda=g$values
output$screeplot <- renderPlot(
plot(y=lambda,
x=1:length(lambda),
log="x",
type="b",
las=1,ylab="Eigenvalues",xlab="Index (log scale)")
)
############
#by Kaiser criterion, the number of Principal Components that should be considered
#factors with eigenvalues greater than 1
#output for the app:
Kaiser <- sum(table_exp[,1] > 1)
output$kaiser <- renderTable({
Kaiser
})
############
#by Person criterion (!!I have not found this anywhere but in my notes!!),
#accept PC until when cumulated variance explained is above 0.8, included
#output for the app:
Pearson <- sum(table_exp[,3] <= 0.8)+1
output$pearson <- renderTable({
Pearson
})
############
#15 rows, 10 columns, just a sample...explain this in the text!
A.std<-scale(A, center=TRUE, scale=TRUE)
scores<-A.std%*%g$vectors
#15 rows, 10 columns, just a sample...explain this in the text!
loadings<-cor(scores, A.std)
#output for the app:
output$loadings <- renderTable({
loadings[1:15,1:10]
})
############
#scatterplot of the scores PC1 versus PC2
#output for the app:
output$PC12plot <- renderPlot({
plot(scores[,1:2])
abline(h=0); abline(v=0)
})
########################################
#a temp file to save the output
outfile <- tempfile(fileext = ".jpg")
# just an easy way to initialize it to a 3D matrix of the correct size
img=original
img[,,1]=R%*%Rv%*%t(Rv)
img[,,2]=G%*%Gv%*%t(Gv)
img[,,3]=B%*%Bv%*%t(Bv)
writeJPEG(img, target = outfile)
#return a list containing information about the image
list(src = outfile,
contentType = "image/jpeg",
alt = "This is alternate text") #I dont know yet what this does but might be useful later
})
#output$files <- renderTable(input$files)
}
shinyApp(ui = ui, server = server)
############################################################
#	IMPLEMENTING PROJECT IN SHINY				                     #
#	last: bea,   20151110                       					     #
############################################################
#-------------------IMPORTANT------------------#
#after finishing the project we will deploy it again:
#shinyapps::deployApp('C:/Users/beyka/Desktop/MAA projects/DA-Project/DA-project/SHINY')
############################################
## app.R ##
library(shiny)
#install.packages("shinydashboard")
library(shinydashboard)
library(jpeg)
# Set Working Directory# ###SET TO YOUR  OWN GITHUB DIRECTORY!!!###
setwd("C:\\Users\\-Andris\\Documents\\GitHub\\DA-project\\SHINY")
#setwd("C:\\Users\\closer\\Documents\\Projecto\\DA-project\\SHINY")
ui <- dashboardPage(
##### HEADER #####
dashboardHeader(title = "Image reconstruction with Principal Component Analysis"),
dashboardSidebar(
width = 300,
sidebarMenu(
menuItem("Picture decomposition", tabName = "home", badgeLabel = "pic", badgeColor ="maroon",icon = icon("line-chart")),
menuItem("Scree plot", tabName = "scree", badgeLabel = "1. step", badgeColor ="green",icon = icon("line-chart")),
menuItem("Loadings", tabName = "loadings", badgeLabel = "2. step", badgeColor ="green", icon = icon("th")),
menuItem("BiPlot", tabName = "biplot", badgeLabel = "3. step", badgeColor ="green", icon = icon("anchor")),
menuItem("Explained Variance", tabName = "tableExplained", badgeLabel = "3. step", badgeColor ="green", icon = icon("anchor")),
menuItem("Choose image",icon = icon("picture", lib = "glyphicon"),
selectInput("image",label = "Choose image",
choices = c("Lisbon" = "lisbon", "Mona Lisa" = "monalisa", "User Input" = "user"))),
menuItem("Choose number of PCs",icon=icon("arrows-h"),
sliderInput(
inputId = "numPCs",
label = "Choose number of PCs",
value = 40, min = 1, max = 100)),
menuItem("Upload your own picture",icon = icon("cloud-upload", lib = "glyphicon"),
fileInput("upload",
label = 'Select an Image',
multiple = FALSE,
accept=c('image/jpeg'))),
menuItem("Choose correlation or covariance",icon = icon("cog", lib = "glyphicon"),
selectInput("CorCov",label = "Choose correlation or covariance",
choices = c("Correlation" = "cor", "Covariance" = "cov"),
multiple = FALSE)),
menuItem("Choose Red Green or Blue",icon = icon("adjust", lib = "glyphicon"),
selectInput("RGB",label = "Choose the component of RGB you want to see a theoretical
explication for",
choices = c("Red" = "r", "Green" = "g", "Blue" = "b"),
multiple = FALSE))
)
),
dashboardBody(
tabItems(
tabItem(tabName = "home",
# Boxes need to be put in a row (or column)
fluidRow(
box(title="Image decomposition", width=12,solidHeader = TRUE, status = "primary",
imageOutput("result"), height = 800)
)
),
##END OF TABITEM
tabItem(tabName = "scree",
# Boxes need to be put in a row (or column)
fluidRow(
box(
title = "Scree plot",width=12,solidHeader = TRUE, status = "primary",
plotOutput("screeplot"), "The scree plot is a useful visual aid for determining an
appropriate number of principal components. The scree plot graphs the eigenvalue against the
component number. To determine the appropriate number of components, we look for an elbow
in the scree plot. The component number is taken to be the point at which the remaining eigenvalues
are relatively small and all about the same size."
)
)
),
##END OF TABITEM
tabItem(tabName = "loadings",
# Boxes need to be put in a row (or column)
fluidRow(
box(
title = "Loadings",width=12,solidHeader = TRUE, status = "primary",
tableOutput("loadings"),"
The Loading Plot is a plot of the relationship between original variables and subspace dimensions. PC loadings measure the importance of each variable in accounting for the
variability in the PC. It is possible to interpret the first few PCs in terms of 'overall' effect or a 'contrast' between groups of variables based on the
structures of PC loadings."
)
)
),
##END OF TABITEM
tabItem(tabName = "biplot",
# Boxes need to be put in a row (or column)
fluidRow(
box(
title = "Bi-Plot",width=12,solidHeader = TRUE, status = "primary",
plotOutput("PC12plot"), "BiPlot:
The bi-plot shows both the loadings and the scores for two selected components in parallel. Bi-plot display is a visualization technique for investigating
the inter-relationships between the observations and
variables in multivariate data. In PCA, relationships between PC scores and PCA loadings
associated with any two PCs can be illustrated in a bi-plot
display. "
)
)
),
##END OF TABITEM
tabItem(tabName = "tableExplained",
# Boxes need to be put in a row (or column)
fluidRow(
box(
title = "Explained Variance",width=12,solidHeader = TRUE, status = "primary",
plotOutput("table_exp"), "BiPlot:
The bi-plot shows both the loadings and the scores for two selected components in parallel. Bi-plot display is a visualization technique for investigating
the inter-relationships between the observations and
variables in multivariate data. In PCA, relationships between PC scores and PCA loadings
associated with any two PCs can be illustrated in a bi-plot
display. "
),
fluidRow(
box(
title = "Kaiser Criteria",width=6,solidHeader = TRUE, status = "primary",
plotOutput("kaiser"), "BiPlot:
The bi-plot shows both the loadings and the scores for two selected components in parallel. Bi-plot display is a visualization technique for investigating
the inter-relationships between the observations and
variables in multivariate data. In PCA, relationships between PC scores and PCA loadings
associated with any two PCs can be illustrated in a bi-plot
display. "
),
box(
title = "Pearson Criteria",width=6,solidHeader = TRUE, status = "primary",
plotOutput("pearson"), "BiPlot:
The bi-plot shows both the loadings and the scores for two selected components in parallel. Bi-plot display is a visualization technique for investigating
the inter-relationships between the observations and
variables in multivariate data. In PCA, relationships between PC scores and PCA loadings
associated with any two PCs can be illustrated in a bi-plot
display. "
))
)
)
##END OF TABITEM
)
)
)
############################################
server <- function(input, output, session) {
# set file limit to 5mb
options(shiny.maxRequestSize = 5*1024^2)
#copy the file into the pic's folder, save as user.jpg
#currently this is hard coded, we could make it dynamic if we define a list for choices, add in this observe,
#we add the new filename etc. Plus for the var<- switch, we need a list too
observe({
if (is.null(input$upload)) return()
file.remove(paste(getwd(),"pics","UserInput.jpg", sep="/"))
file.copy(input$upload$datapath, paste(getwd(),"pics","UserInput.jpg", sep="/"))
})
output$result <- renderImage({
#Dropdown menu decides which input to load
#Reads the filenames into a variable
picfiles<- grep('.jpg', list.files(path = paste(getwd(),"pics", sep="/")), value=TRUE)
#Rtrail - eliminating .jpg
picnames<-gsub(".jpg", "", picfiles)
#Creating named list for the dropdown menu
ls_choices.list <- as.list(picfiles)
names(ls_choices.list) <- picnames
ls_choices.list
#Updateing the dropdown list with the created named list - not working yet
### !!problem is the condition, that's not following the values - links yet !!###
#updateSelectInput(session, "image", choices = ls_choices.list)
# Change values for input$inSelect
var <-switch(input$image,
"lisbon" = "pics/LisbonInput.jpg",
"monalisa" = "pics/MonaLisaInput.jpg",
"user" = "pics/UserInput.jpg"
)
# loading picture
original=readJPEG(var)
R=original[,,1]
G=original[,,2]
B=original[,,3]
#Updating the maximum number of PCas to use
updateSliderInput(session, "numPCs", max =dim(original)[2])
#output$testing <- renderDataTable(picnames)
# Do the same process separatly for each channel
#R=original[,,1]
#G=original[,,2]
#B=original[,,3]
# Compute a correlation or covariance matrix
# and its eigen vectors
#Number of principal components to use:
k = input$numPCs
### Red
r <-switch(input$CorCov,
"cor" = cor(R),
"cov" = cov(R)
)
g=eigen(r)
Rv=g$vectors[,1:k]
###Green
r <-switch(input$CorCov,
"cor" = cor(G),
"cov" = cov(G)
)
g=eigen(r)
Gv=g$vectors[,1:k]
###Blue
r <-switch(input$CorCov,
"cor" = cor(B),
"cov" = cov(B)
)
g=eigen(r)
Bv=g$vectors[,1:k]
###########################
#output theory for R,G or B
A <- switch(input$RGB,
"r" = R,
"g" = G,
"b" = B)
g <- switch(input$CorCov,
"cor" = eigen(cor(A)),
"cov" = eigen(cov(A)))
#perentage of total variation explained by the components
perc_exp<-g$values/NCOL(A)
cum_exp<-c(perc_exp[1], sum(perc_exp[1:2]), sum(perc_exp[1:3]),
sum(perc_exp[1:4]),sum(perc_exp[1:5]),sum(perc_exp[1:6]),
sum(perc_exp[1:7]),sum(perc_exp[1:8]),sum(perc_exp[1:9]),
sum(perc_exp[1:10]),sum(perc_exp[1:11]),sum(perc_exp[1:12]),
sum(perc_exp[1:13]),sum(perc_exp[1:14]),sum(perc_exp[1:15]))
table_exp<-cbind(values=g$values[1:15], variance_explained=perc_exp[1:15],
cummulated_variance_explained=cum_exp)
#output for the app:
output$table_exp <- renderTable({
table_exp
})
#just for testing the variable, can be deleted in the end
#writing location
output$testing <- renderPlot({
plot(g$values, type='b')
})
############
#screeplot to see the elbow
#output for the app:
lambda=g$values
output$screeplot <- renderPlot(
plot(y=lambda,
x=1:length(lambda),
log="x",
type="b",
las=1,ylab="Eigenvalues",xlab="Index (log scale)")
)
############
#by Kaiser criterion, the number of Principal Components that should be considered
#factors with eigenvalues greater than 1
#output for the app:
Kaiser <- sum(table_exp[,1] > 1)
output$kaiser <- renderTable({
Kaiser
})
############
#by Person criterion (!!I have not found this anywhere but in my notes!!),
#accept PC until when cumulated variance explained is above 0.8, included
#output for the app:
Pearson <- sum(table_exp[,3] <= 0.8)+1
output$pearson <- renderTable({
Pearson
})
############
#15 rows, 10 columns, just a sample...explain this in the text!
A.std<-scale(A, center=TRUE, scale=TRUE)
scores<-A.std%*%g$vectors
#15 rows, 10 columns, just a sample...explain this in the text!
loadings<-cor(scores, A.std)
#output for the app:
output$loadings <- renderTable({
loadings[1:15,1:10]
})
############
#scatterplot of the scores PC1 versus PC2
#output for the app:
output$PC12plot <- renderPlot({
plot(scores[,1:2])
abline(h=0); abline(v=0)
})
########################################
#a temp file to save the output
outfile <- tempfile(fileext = ".jpg")
# just an easy way to initialize it to a 3D matrix of the correct size
img=original
img[,,1]=R%*%Rv%*%t(Rv)
img[,,2]=G%*%Gv%*%t(Gv)
img[,,3]=B%*%Bv%*%t(Bv)
writeJPEG(img, target = outfile)
#return a list containing information about the image
list(src = outfile,
contentType = "image/jpeg",
alt = "This is alternate text") #I dont know yet what this does but might be useful later
})
#output$files <- renderTable(input$files)
}
shinyApp(ui = ui, server = server)
# Set Working Directory
setwd("C:/Users/-Andris/Documents/GitHub/DA-project/SHINY/pics")
# (Install jpeg library previously)
# Load library and read the file
library(jpeg)
original = readJPEG("MonaLisaInput.jpg")
# The image is viewed as a 3D array of dimensions:
dim(original)
# That means to each pixel corresponds 3 values, that
# determine the color (RGB)
# RGB channels can have values between 0 and 255 or
# between 0 and 1 (which is the case)
# To do the analysis we need a matrix with the same number
# of pixels but with just one value per pixel
# To do that we can use just one channel or combine the 3
# Lets start with gray images.A possible way of converting it:
gray = (original[,,1]+original[,,2]+original[,,3])/3
# Do the analysis using the correlation or the covariance
# matrix. Just change the definition of r
#r=cov(gray)
r=cor(gray)
g=eigen(r)
v=g$vectors
# Do the scree plot (using log scale just to see it better)
lambda=g$values
plot(y=lambda,
x=1:length(lambda),
log="x",
type="b",
las=1,ylab="Eigenvalues",xlab="Index (log scale)")
# Reconstruct the original image
img=gray%*%v%*%t(v)
writeJPEG(img,"original.jpeg")
# Reconstruct the image with just k pcs
k=7
v=g$vectors[,1:k]
img=gray%*%v%*%t(v)
name=paste(k,"PCs.jpeg")
writeJPEG(img,name)
# If you want to see how a particular component looks like
k=1
v=g$vectors[,k]
img=gray%*%v%*%t(v)
name=paste(k,"PC.jpeg")
writeJPEG(img,name)
perc_exp<-g$values/NCOL(A)
cum_exp<-c(perc_exp[1], sum(perc_exp[1:2]), sum(perc_exp[1:3]),
sum(perc_exp[1:4]),sum(perc_exp[1:5]),sum(perc_exp[1:6]),
sum(perc_exp[1:7]),sum(perc_exp[1:8]),sum(perc_exp[1:9]),
sum(perc_exp[1:10]),sum(perc_exp[1:11]),sum(perc_exp[1:12]),
sum(perc_exp[1:13]),sum(perc_exp[1:14]),sum(perc_exp[1:15]))
table_exp<-cbind(values=g$values[1:15], variance_explained=perc_exp[1:15],
cummulated_variance_explained=cum_exp)
Kaiser <- sum(table_exp[,1] > 1)
perc_exp<-g$values/NCOL(g$values)
perc_exp
perc_exp
perc_exp<-g$values/NCOL(g)
perc_exp
perc_exp<-g$values/NCOL(g)
rm(perc_exp)
perc_exp
perc_exp<-g$values/NCOL(g)
perc_exp
g
g
header(g)
head(g)
ncol(g)
NCOL(g)
g
NCOL(g)
g
NROW(g)
g
g
NROW(g)
dim(original)
